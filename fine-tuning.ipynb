{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10862011,"sourceType":"datasetVersion","datasetId":6747667}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndf = load_dataset(\"parquet\", data_files=\"/kaggle/input/financial-qa/0000.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:05:28.115235Z","iopub.execute_input":"2025-02-27T10:05:28.115567Z","iopub.status.idle":"2025-02-27T10:05:28.921389Z","shell.execute_reply.started":"2025-02-27T10:05:28.115506Z","shell.execute_reply":"2025-02-27T10:05:28.920661Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:05:28.922466Z","iopub.execute_input":"2025-02-27T10:05:28.922883Z","iopub.status.idle":"2025-02-27T10:05:28.929468Z","shell.execute_reply.started":"2025-02-27T10:05:28.922855Z","shell.execute_reply":"2025-02-27T10:05:28.928591Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['system', 'user', 'assistant'],\n        num_rows: 454234\n    })\n})\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"df_train_test = df['train'].train_test_split(test_size=0.05)\ndf_val_test = df_train_test['train'].train_test_split(test_size=0.05)\n\ntrain_dataset = df_val_test[\"train\"]\nval_dataset = df_val_test[\"test\"]\ntest_dataset = df_train_test[\"test\"]\n\nprint(f\"Train Dataset: {train_dataset.num_rows}\")\nprint(f\"Validation Dataset: {val_dataset.num_rows}\")\nprint(f\"Test Dataset: {test_dataset.num_rows}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:05:28.930633Z","iopub.execute_input":"2025-02-27T10:05:28.930872Z","iopub.status.idle":"2025-02-27T10:05:29.202542Z","shell.execute_reply.started":"2025-02-27T10:05:28.930849Z","shell.execute_reply":"2025-02-27T10:05:29.201721Z"}},"outputs":[{"name":"stdout","text":"Train Dataset: 409945\nValidation Dataset: 21577\nTest Dataset: 22712\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(train_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:05:29.203732Z","iopub.execute_input":"2025-02-27T10:05:29.203961Z","iopub.status.idle":"2025-02-27T10:05:29.208470Z","shell.execute_reply.started":"2025-02-27T10:05:29.203941Z","shell.execute_reply":"2025-02-27T10:05:29.207695Z"}},"outputs":[{"name":"stdout","text":"['system', 'user', 'assistant']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n\ndef preprocess_function(examples):\n    inputs = [f\"question: {q}\" for q in examples[\"user\"]]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n\n    labels = tokenizer(examples[\"assistant\"], max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:05:29.209295Z","iopub.execute_input":"2025-02-27T10:05:29.209527Z","iopub.status.idle":"2025-02-27T10:05:29.533185Z","shell.execute_reply.started":"2025-02-27T10:05:29.209493Z","shell.execute_reply":"2025-02-27T10:05:29.532474Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:05:29.533847Z","iopub.execute_input":"2025-02-27T10:05:29.534052Z","iopub.status.idle":"2025-02-27T10:13:38.448482Z","shell.execute_reply.started":"2025-02-27T10:05:29.534034Z","shell.execute_reply":"2025-02-27T10:13:38.447758Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/409945 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ae612a3961f446bb750b6f41ff32347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21577 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1214fc147151441c9ebca680713948f4"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, Trainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\nimport os\n\n# Load T5 model\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\nos.environ[\"WANDB_DISABLED\"] = \"false\"\n\n# Training Arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5_finetuned\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    eval_strategy=\"steps\",\n    eval_steps=2000,\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    save_strategy=\"steps\",\n    save_steps=2000,\n    learning_rate=4e-5,\n    weight_decay=0.01,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16=True,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    run_name=\"run 1\"\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:36:14.962017Z","iopub.execute_input":"2025-02-27T10:36:14.962317Z","iopub.status.idle":"2025-02-27T10:36:15.531768Z","shell.execute_reply.started":"2025-02-27T10:36:14.962294Z","shell.execute_reply":"2025-02-27T10:36:15.531005Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# import shutil\n# import os\n\n# # Path to the output directory\n# folder_path = \"/kaggle/working/t5_finetuned\"\n\n# # Remove the folder if it exists\n# if os.path.exists(folder_path):\n#     shutil.rmtree(folder_path)\n#     print(f\"Deleted {folder_path}\")\n# else:\n#     print(f\"{folder_path} does not exist.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:34:16.424002Z","iopub.execute_input":"2025-02-27T10:34:16.424315Z","iopub.status.idle":"2025-02-27T10:34:16.429788Z","shell.execute_reply.started":"2025-02-27T10:34:16.424292Z","shell.execute_reply":"2025-02-27T10:34:16.428981Z"}},"outputs":[{"name":"stdout","text":"Deleted /kaggle/working/t5_finetuned\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:36:24.125578Z","iopub.execute_input":"2025-02-27T10:36:24.125880Z","iopub.status.idle":"2025-02-27T10:36:24.229218Z","shell.execute_reply.started":"2025-02-27T10:36:24.125858Z","shell.execute_reply":"2025-02-27T10:36:24.228575Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:36:32.729277Z","iopub.execute_input":"2025-02-27T10:36:32.729605Z","iopub.status.idle":"2025-02-27T14:35:50.936665Z","shell.execute_reply.started":"2025-02-27T10:36:32.729580Z","shell.execute_reply":"2025-02-27T14:35:50.935937Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25622' max='25622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25622/25622 3:59:17, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2000</td>\n      <td>1.017700</td>\n      <td>0.905030</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.939600</td>\n      <td>0.855493</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.894800</td>\n      <td>0.831168</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.922100</td>\n      <td>0.814644</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.873000</td>\n      <td>0.804346</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.872300</td>\n      <td>0.796109</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.848000</td>\n      <td>0.790042</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.877000</td>\n      <td>0.785454</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.862000</td>\n      <td>0.781721</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.858400</td>\n      <td>0.779103</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.840600</td>\n      <td>0.777363</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.845800</td>\n      <td>0.776307</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=25622, training_loss=0.9067224126334456, metrics={'train_runtime': 14357.5368, 'train_samples_per_second': 28.553, 'train_steps_per_second': 1.785, 'total_flos': 5.548269480443904e+16, 'train_loss': 0.9067224126334456, 'epoch': 1.0})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nmodel_path = \"/kaggle/working/t5_finetuned/checkpoint-25622\"\n\ntrained_model = T5ForConditionalGeneration.from_pretrained(model_path)\n\ndef generate_answer(question):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    trained_model.to(device)  # Move model to the same device\n    input_text = f\"question: {question}\"\n\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()} \n\n    # Generate response using trained_model (not model)\n    outputs = trained_model.generate(\n        **inputs,\n        max_length=128,\n        do_sample=True, \n        temperature=0.7, \n        top_k=50,  \n        top_p=0.9,  \n        repetition_penalty=1.2,  \n        no_repeat_ngram_size=4,  \n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return response\n\nquestion = \"what is an investment?\"\nanswer = generate_answer(question)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Define your folder path\nfolder_path = \"/kaggle/working/t5_finetuned/checkpoint-25622\"\n\n# Create a zip archive of the folder\nshutil.make_archive(\"/kaggle/working/t5_finetuned\", 'zip', folder_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:56:22.373155Z","iopub.execute_input":"2025-02-27T14:56:22.373491Z","iopub.status.idle":"2025-02-27T14:56:57.472889Z","shell.execute_reply.started":"2025-02-27T14:56:22.373460Z","shell.execute_reply":"2025-02-27T14:56:57.472130Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/t5_finetuned.zip'"},"metadata":{}}],"execution_count":48}]}